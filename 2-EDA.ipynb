{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet Water Level Outliers\n",
    "\n",
    "Use water level measurements of groundwater available from IntellusNM.com to identify outliers, trend, seasonality, and upset events.\n",
    "\n",
    "This notebook contains basic statistical analysis and visualization of the data.\n",
    "\n",
    "### Data Sources\n",
    "- summary : Processed file from notebook 1-Data_Prep\n",
    "\n",
    "### Changes\n",
    "- 02-19-2024 : Started project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/0j2ns2q56n7_rxycqh03plxm0000gn/T/ipykernel_80566/211503038.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/paulmark/JupyterNotebooks/Water Level Outliers/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import prophet\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today()\n",
    "in_file = Path.cwd() / \"data\" / \"processed\" / f\"summary_{today:%b-%d-%Y}.pkl\"\n",
    "report_dir = Path.cwd() / \"reports\"\n",
    "report_file = report_dir / \"Excel_Analysis_{today:%b-%d-%Y}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Site ID                                             object\n",
       "Location ID                                         object\n",
       "ds                                          datetime64[ns]\n",
       "Groundwater Measurement                            float64\n",
       "y                                                  float64\n",
       "Groundwater Level Comments                          object\n",
       "Groundwater Level Data Quality Code                 object\n",
       "Groundwater Level Validation Reason Code            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(in_file)\n",
    "df = df.rename(columns={'Measurement Date Time':'ds', 'Groundwater Elevation':'y'})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: prophet\n",
      "Version: 1.1.5\n",
      "Summary: Automatic Forecasting Procedure\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Sean J. Taylor\" <sjtz@pm.me>, Ben Letham <bletham@fb.com>\n",
      "License: MIT\n",
      "Location: /Users/paulmark/JupyterNotebooks/Water Level Outliers/.venv/lib/python3.11/site-packages\n",
      "Requires: cmdstanpy, holidays, importlib-resources, matplotlib, numpy, pandas, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Data Analysis - Loop through locations to identify anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = df[['Location ID','Site ID']].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:39:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:29 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "markers = {'N':'X', 'Y':'o'}\n",
    "hue_order = ['N','Y']\n",
    "style_order = ['N','Y']\n",
    "iw = 0.99\n",
    "\n",
    "\n",
    "for location, site_id in zip(location['Location ID'], location['Site ID']):\n",
    "\t# Add seasonality and instantiate a new Prophet model\n",
    "\tmodel = Prophet(interval_width=iw, yearly_seasonality=True, weekly_seasonality=True)\n",
    "\n",
    "\t# print(location, parameter)\n",
    "\texport_subset = df[(df['Location ID'] == location) & (df['Site ID'] == site_id)]\n",
    "\t\n",
    "\texport_subset = export_subset[export_subset.groupby(['Location ID']).transform('size')>10]\n",
    "\n",
    "\tif export_subset.empty:\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Fit the model on the training dataset\n",
    "\tmodel.fit(export_subset)\n",
    "\n",
    "\t# Make prediction\n",
    "\tforecast = model.predict(export_subset)\n",
    "\n",
    "\t# Merge actual and predicted values\n",
    "\tperformance = pd.merge(export_subset, forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds')\n",
    "\n",
    "\t# Create an anomaly indicator\n",
    "\tperformance['anomaly'] = performance.apply(lambda rows: 1 if ((rows.y<rows.yhat_lower)|(rows.y>rows.yhat_upper)) else 0, axis = 1)\n",
    "\n",
    "\tanomalies = performance[performance['anomaly']==1].sort_values(by='ds')\n",
    "\tif anomalies.empty:\n",
    "\t\tcontinue\n",
    "\t\n",
    "\tanomalies.to_csv('anomalies.csv', mode='a', index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Excel file into reports directory\n",
    "\n",
    "Save an Excel file with intermediate results into the report directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(report_file, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(writer, sheet_name='Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
